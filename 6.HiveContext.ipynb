{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Hive-Data-Definition-Language-(DDL)\" data-toc-modified-id=\"Hive-Data-Definition-Language-(DDL)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Hive Data Definition Language (DDL)</a></span></li><li><span><a href=\"#Data-SELECT-and-JOIN\" data-toc-modified-id=\"Data-SELECT-and-JOIN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data <code>SELECT</code> and <code>JOIN</code></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Nested-SELECT\" data-toc-modified-id=\"Nested-SELECT-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Nested <code>SELECT</code></a></span></li><li><span><a href=\"#Aggregation-and-sampling\" data-toc-modified-id=\"Aggregation-and-sampling-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Aggregation and sampling</a></span></li></ul></li></ul></li><li><span><a href=\"#Analytic-functions\" data-toc-modified-id=\"Analytic-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Analytic functions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:24:38.015744Z",
     "start_time": "2021-12-07T13:24:28.003385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '11163'),\n",
       " ('spark.app.name', 'test'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.199.77'),\n",
       " ('spark.hadoop.fs.default.name', 'hdfs://localhost:9000'),\n",
       " ('spark.app.id', 'local-1638883476000'),\n",
       " ('spark.sql.warehouse.dir', '/user/hive/warehouse'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.hadoop.fs.defaultFS', 'hdfs://localhost:9000'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.startTime', '1638883474378'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.\n",
    "    appName(\"test\").\n",
    "    enableHiveSupport().\n",
    "    config(\"spark.sql.warehouse.dir\",\"/user/hive/warehouse\").\n",
    "    getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:27:42.475440Z",
     "start_time": "2021-12-07T13:27:41.506848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"station_id\",\"bikes_available\",\"docks_available\",\"time\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:00:02\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:01:03\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:02:03\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:03:02\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:04:03\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:05:02\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:06:03\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:07:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:08:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:09:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:10:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:11:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:12:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:13:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:14:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:15:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:16:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:17:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:18:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:19:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:20:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:21:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:22:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:23:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:24:02\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:25:03\"\r\n",
      "\"2\",\"13\",\"14\",\"2014-03-01 00:26:"
     ]
    }
   ],
   "source": [
    "path = \"file:///home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_status_data.csv\"\n",
    "!hdfs dfs -head {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:42:05.045834Z",
     "start_time": "2021-12-07T13:42:05.019692Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- bikes_available: integer (nullable = true)\n",
      " |-- docks_available: integer (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"station_id int, bikes_available int, docks_available int, time timestamp\"\n",
    "df = spark.read.csv(path, header=True, schema=schema).cache()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:30:59.465274Z",
     "start_time": "2021-12-07T13:29:56.465177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18342210"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|             12|             15|2014-03-01 00:00:02|\n",
      "|         2|             12|             15|2014-03-01 00:01:03|\n",
      "|         2|             12|             15|2014-03-01 00:02:03|\n",
      "|         2|             12|             15|2014-03-01 00:03:02|\n",
      "|         2|             12|             15|2014-03-01 00:04:03|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- bikes_available: string (nullable = true)\n",
      " |-- docks_available: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|(2 * bikes_available)|\n",
      "+---------------------+\n",
      "|                 24.0|\n",
      "|                 24.0|\n",
      "|                 24.0|\n",
      "|                 24.0|\n",
      "|                 24.0|\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(('2*bikes_available')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work!!! :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:32:18.692960Z",
     "start_time": "2021-12-07T13:32:18.688839Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.select(('2*bikes_available')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|2 x bikes_available|\n",
      "+-------------------+\n",
      "|                 24|\n",
      "|                 24|\n",
      "|                 24|\n",
      "|                 24|\n",
      "|                 24|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "bikes_double = udf(lambda x: 2*int(x), IntegerType())\n",
    "df.select(bikes_double('bikes_available').alias('2 x bikes_available')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:34:33.634651Z",
     "start_time": "2021-12-07T13:34:33.234825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|(bikes_available * 2)|\n",
      "+---------------------+\n",
      "|                   24|\n",
      "|                   24|\n",
      "|                   24|\n",
      "|                   24|\n",
      "|                   24|\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(2*df.bikes_available).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Data Definition Language (DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:43:30.508132Z",
     "start_time": "2021-12-07T13:43:30.455398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|station_id     |int      |null   |\n",
      "|bikes_available|int      |null   |\n",
      "|docks_available|int      |null   |\n",
      "|time           |timestamp|null   |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.dropTempView('tb1')\n",
    "df.createTempView('tb1')\n",
    "spark.sql(\"DESCRIBE FORMATTED tb1\").show(truncate=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:35:43.232642Z",
     "start_time": "2021-12-07T13:35:40.106803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      namespace|\n",
      "+---------------+\n",
      "|            cbr|\n",
      "|        default|\n",
      "|hive_essentials|\n",
      "|    pluralsight|\n",
      "|         rostel|\n",
      "|    rostel_test|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:36:25.994382Z",
     "start_time": "2021-12-07T13:36:25.850894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------------+\n",
      "|info_name    |info_value                               |\n",
      "+-------------+-----------------------------------------+\n",
      "|Database Name|default                                  |\n",
      "|Comment      |Default Hive database                    |\n",
      "|Location     |hdfs://localhost:9000/user/hive/warehouse|\n",
      "|Owner        |public                                   |\n",
      "+-------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE DATABASE default').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:36:57.841329Z",
     "start_time": "2021-12-07T13:36:57.822339Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('USE default');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|tableName|isTemporary|\n",
      "+---------+-----------+\n",
      "|      tb1|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- bikes_available: string (nullable = true)\n",
      " |-- docks_available: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|     station_id|   string|       |\n",
      "|bikes_available|   string|       |\n",
      "|docks_available|   string|       |\n",
      "|           time|   string|       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context.sql('desc tb1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data `SELECT` and `JOIN`  \n",
    "\n",
    "The `SELECT` statement is quite often used with `FROM , DISTINCT , WHERE , and LIMIT`\n",
    "keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:38:45.568781Z",
     "start_time": "2021-12-07T13:38:45.350858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|             12|             15|2014-03-01 00:00:02|\n",
      "|         2|             12|             15|2014-03-01 00:01:03|\n",
      "|         2|             12|             15|2014-03-01 00:02:03|\n",
      "|         2|             12|             15|2014-03-01 00:03:02|\n",
      "|         2|             12|             15|2014-03-01 00:04:03|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from tb1 where bikes_available >= 10').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:38:54.786122Z",
     "start_time": "2021-12-07T13:38:53.383633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|number_of_dockstations|\n",
      "+----------------------+\n",
      "|                    70|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(distinct station_id) number_of_dockstations from tb1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:44:46.703117Z",
     "start_time": "2021-12-07T13:44:46.143000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|dist_sid|\n",
      "+--------+\n",
      "|      84|\n",
      "|      83|\n",
      "|      82|\n",
      "|      80|\n",
      "|      77|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct station_id as dist_sid from tb1 order by dist_sid desc limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:44:53.932415Z",
     "start_time": "2021-12-07T13:44:53.748315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|max(station_id)|\n",
      "+---------------+\n",
      "|             84|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select max(station_id) from tb1;').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested `SELECT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:45:36.435431Z",
     "start_time": "2021-12-07T13:45:36.254868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|        80|              8|              7|2014-03-01 00:00:02|\n",
      "|        80|              8|              7|2014-03-01 00:01:03|\n",
      "|        80|              8|              7|2014-03-01 00:02:03|\n",
      "|        80|              8|              7|2014-03-01 00:03:02|\n",
      "|        80|              8|              7|2014-03-01 00:04:03|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    WITH tb2 AS\n",
    "    (SELECT * FROM tb1 WHERE station_id >=80)\n",
    "    SELECT * from tb2 LIMIT 5\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|        80|              8|              7|2014-03-01 00:00:02|\n",
      "|        80|              8|              7|2014-03-01 00:01:03|\n",
      "|        80|              8|              7|2014-03-01 00:02:03|\n",
      "|        80|              8|              7|2014-03-01 00:03:02|\n",
      "|        80|              8|              7|2014-03-01 00:04:03|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM (SELECT * FROM tb1 WHERE station_id >= 80) tb2').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:50:21.789897Z",
     "start_time": "2021-12-07T13:50:21.373908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|maxDayOfMonth|\n",
      "+-------------+\n",
      "|           31|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select max(day(TIME)) maxDayOfMonth \n",
    "from tb1'''\n",
    "           ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime format specifications:\n",
    "\n",
    "http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:54:27.857034Z",
     "start_time": "2021-12-07T13:54:27.389509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|MAX_DOW|\n",
      "+-------+\n",
      "|      7|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select max(dayOfWeek(time)) as MAX_DOW from tb1;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:54:38.393588Z",
     "start_time": "2021-12-07T13:54:38.390255Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql('''\n",
    "# select max(from_unixtime(unix_timestamp(TIME,\"yyyy-MM-dd\"),\"u\")) maxDayOfWeek\n",
    "# from tb1\n",
    "# '''\n",
    "#            ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:55:19.869441Z",
     "start_time": "2021-12-07T13:55:19.026660Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 47:==========================================>              (9 + 3) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|DoW|\n",
      "+---+\n",
      "|  1|\n",
      "|  6|\n",
      "|  3|\n",
      "|  5|\n",
      "|  4|\n",
      "|  7|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct dayOfWeek(time) DoW from tb1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:56:08.399304Z",
     "start_time": "2021-12-07T13:56:07.559793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|DoW_SET              |\n",
      "+---------------------+\n",
      "|[1, 5, 2, 6, 3, 7, 4]|\n",
      "+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:==============>                                          (3 + 9) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select collect_set(dayOfWeek(time)) DoW_SET from tb1').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation and sampling\n",
    "\n",
    "` MIN , MAX , AVG , GROUPING SETS, ROLLUP , CUBE `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|        55|              4|             19|2014-04-28 08:50:03|\n",
      "|        42|             11|              4|2014-06-19 04:44:02|\n",
      "|        36|              7|              8|2014-03-15 11:23:02|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM tb1 DISTRIBUTE BY RAND() SORT BY RAND() LIMIT 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:58:10.457014Z",
     "start_time": "2021-12-07T13:58:10.296103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1851"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = spark.sql('SELECT * FROM tb1 TABLESAMPLE(.01 PERCENT)')\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|             14|             12|2014-03-25 00:27:02|\n",
      "|         2|             11|             16|2014-04-02 02:55:02|\n",
      "|         2|              8|             19|2014-04-10 02:24:03|\n",
      "|         2|              9|             18|2014-04-18 06:52:03|\n",
      "|         2|             12|             15|2014-04-20 12:36:02|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:58:19.510852Z",
     "start_time": "2021-12-07T13:58:19.251950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "| default|     employee|      false|\n",
      "| default|  employee_07|      false|\n",
      "| default|      licence|      false|\n",
      "| default|logs_category|      false|\n",
      "| default|     students|      false|\n",
      "| default|  ward_counts|      false|\n",
      "|        |          tb1|       true|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T13:58:31.487097Z",
     "start_time": "2021-12-07T13:58:29.512514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:====>                                                   (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|             11|             16|2014-03-10 02:14:02|\n",
      "|         5|              8|             11|2014-05-14 21:04:02|\n",
      "|         2|             17|             10|2014-05-15 01:13:02|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:======================================>                  (8 + 4) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT * \n",
    "               FROM tb1 \n",
    "               SORT BY RAND() \n",
    "               LIMIT 3''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|free_slots|\n",
      "+----------+\n",
      "|       3.0|\n",
      "|       3.0|\n",
      "|       3.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "               SELECT docks_available - bikes_available AS free_slots\n",
    "               FROM tb1\n",
    "               LIMIT 3\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ` SORT BY ` in most cases preferrable to ` ORDER BY ` because it's faster (due to use of many reducers in ` SORT BY `)    \n",
    "\n",
    "\n",
    "- Always avoid using ` ORDER BY ` in queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T14:01:24.653098Z",
     "start_time": "2021-12-07T14:01:23.798832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|DoW|average_free_slots|\n",
      "+---+------------------+\n",
      "|  1|              0.67|\n",
      "|  2|              0.84|\n",
      "|  3|              0.92|\n",
      "|  4|              0.86|\n",
      "|  5|              0.82|\n",
      "|  6|              0.77|\n",
      "|  7|              0.65|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT dayofweek(time) DoW,\n",
    "               round(AVG(docks_available - bikes_available),2) average_free_slots\n",
    "               FROM tb1\n",
    "               GROUP BY dayofweek(time)\n",
    "               ORDER BY dayofweek(time) ASC\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T14:04:01.453342Z",
     "start_time": "2021-12-07T14:04:00.105673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|DoW|avearge_free_slots|\n",
      "+---+------------------+\n",
      "|Sun|              0.67|\n",
      "|Mon|              0.84|\n",
      "|Thu|              0.82|\n",
      "|Sat|              0.65|\n",
      "|Wed|              0.86|\n",
      "|Tue|              0.92|\n",
      "|Fri|              0.77|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT date_format(time, 'E') DoW,\n",
    "             round(AVG(docks_available - bikes_available),2) avearge_free_slots\n",
    "             FROM tb1\n",
    "             GROUP BY date_format(time, 'E')\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T14:04:53.931476Z",
     "start_time": "2021-12-07T14:04:52.666861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 87:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|day|  count|\n",
      "+---+-------+\n",
      "|Sun|2634647|\n",
      "|Mon|2611193|\n",
      "|Thu|2612085|\n",
      "|Sat|2662475|\n",
      "|Wed|2611430|\n",
      "|Tue|2610988|\n",
      "|Fri|2599392|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT date_format(time,\"E\") as day,\n",
    "             count(*) as `count`\n",
    "             FROM tb1\n",
    "             GROUP BY date_format(time,\"E\")''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting columns that are not part of `GROUP BY` statement with:\n",
    "\n",
    "> `colect_set(colName)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T14:19:55.209156Z",
     "start_time": "2021-12-07T14:19:53.221184Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:=========>                                             (2 + 10) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------+\n",
      "|dayID|  DoW|average_free_slots|\n",
      "+-----+-----+------------------+\n",
      "|    1|[Sun]|              0.67|\n",
      "|    2|[Mon]|              0.84|\n",
      "|    3|[Tue]|              0.92|\n",
      "|    4|[Wed]|              0.86|\n",
      "|    5|[Thu]|              0.82|\n",
      "|    6|[Fri]|              0.77|\n",
      "|    7|[Sat]|              0.65|\n",
      "+-----+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT dayofweek(time) dayID,\n",
    "             collect_set(date_format(time, 'E')) DoW,\n",
    "             round(AVG(docks_available - bikes_available), 2) average_free_slots\n",
    "             FROM tb1\n",
    "             GROUP BY dayofweek(time)\n",
    "             ORDER BY dayofweek(time) ASC\n",
    "             ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T14:28:06.545340Z",
     "start_time": "2021-12-07T14:28:05.873536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|  DoW|avearge_free_slots|\n",
      "+-----+------------------+\n",
      "|[Sun]|0.6719503599533448|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 113:>                                                      (0 + 12) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "with tb2 as\n",
    "(select * from tb1 where dayofweek(time) = 1)\n",
    "select collect_set(date_format(time, 'E')) DoW,\n",
    "       AVG(docks_available - bikes_available) avearge_free_slots \n",
    "from tb2\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic functions\n",
    "\n",
    "Used together with ` OVER , PARTITION BY , ORDER BY `  \n",
    "\n",
    "Though analytic functions give aggregate results, they\n",
    "do not group the result set. They return the group value multiple times with each\n",
    "record.  \n",
    "\n",
    "\n",
    "` Function (arg1,..., argn) OVER ([PARTITION BY <...>] [ORDER BY <....>]\n",
    "[<window_clause>]) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         last(time)|\n",
      "+-------------------+\n",
      "|2014-08-31 23:59:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context.sql(\"select last_value(time) from tb1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          max(time)|\n",
      "+-------------------+\n",
      "|2014-08-31 23:59:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context.sql(\"select max(time) from tb1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert table to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = context.table('tb1')\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|             12|             15|2014-03-01 00:00:02|\n",
      "|         2|             12|             15|2014-03-01 00:01:03|\n",
      "|         2|             12|             15|2014-03-01 00:02:03|\n",
      "|         2|             12|             15|2014-03-01 00:03:02|\n",
      "|         2|             12|             15|2014-03-01 00:04:03|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "104px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
