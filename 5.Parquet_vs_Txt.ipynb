{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим содержание и структуру файла, который будем конвертировать в формат `parquet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"station_id\",\"bikes_available\",\"docks_available\",\"time\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:00:02\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:01:03\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:02:03\"\r\n",
      "\"2\",\"12\",\"15\",\"2014-03-01 00:03:02\"\r\n"
     ]
    }
   ],
   "source": [
    "! head -n5 /home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_status_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для конвертации нам понадобится определение типов полей (\"схема\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customSchema = StructType([\n",
    "        StructField(\"station_id\", IntegerType(),True),\n",
    "        StructField(\"bikes_available\", IntegerType(),True),\n",
    "        StructField(\"docks_available\", IntegerType(),True),\n",
    "        StructField(\"time\", TimestampType(),True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем `csv` файл с указанием схемы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- bikes_available: integer (nullable = true)\n",
      " |-- docks_available: integer (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readPath = \"file:///home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_status_data.csv\"\n",
    "df = spark.read.csv(readPath, header=True, schema=customSchema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем записать в Hadoop сначала `csv` файл в простом текстовом формате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txtPath = \"hdfs:///user/sergey/txt\"\n",
    "df.write.csv(txtPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... затем тот же самый файл в формате `parquet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parquetPath = \"hdfs:///user/sergey/parquet\"\n",
    "df.write.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объем исходного файла в локальной файловой систему Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623M\t/home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_status_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "! du -h /home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_status_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объем записанных файлов в Hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.2 M  parquet\r\n",
      "430.4 M  txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/09/10 22:35:43 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "station_id = 2\n",
      "bikes_available = 12\n",
      "docks_available = 15\n",
      "time = ABSiRXxBAACNfCUA\n",
      "\n",
      "station_id = 2\n",
      "bikes_available = 12\n",
      "docks_available = 15\n",
      "time = ADaEeYpBAACNfCUA\n",
      "\n",
      "station_id = 2\n",
      "bikes_available = 12\n",
      "docks_available = 15\n",
      "time = AI7LcZhBAACNfCUA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar ~/tools/parquet-tools.jar head -n3 /user/sergey/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+--------------------+\n",
      "|station_id|bikes_available|docks_available|                time|\n",
      "+----------+---------------+---------------+--------------------+\n",
      "|         2|             12|             15|2014-03-01 00:00:...|\n",
      "|         2|             12|             15|2014-03-01 00:01:...|\n",
      "|         2|             12|             15|2014-03-01 00:02:...|\n",
      "|         2|             12|             15|2014-03-01 00:03:...|\n",
      "|         2|             12|             15|2014-03-01 00:04:...|\n",
      "+----------+---------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"hdfs:///user/sergey/parquet\")\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданный `parquet` файл может быть загружен в `Hive` при помощи следующих эквивалентных выражений:\n",
    "\n",
    "\n",
    "> `CREATE EXTERNAL TABLE IF NOT EXISTS bikes_parquet777 (\n",
    "id_station BIGINT,\n",
    "bikes_available BIGINT,\n",
    "docks_available BIGINT,\n",
    "time TIMESTAMP)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/user/sergey/parquet';`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">`create external table bikes_parquet_777 (\n",
    "station_id smallint,\n",
    "bikes_available smallint,\n",
    "docks_available smallint,\n",
    "time timestamp)\n",
    "stored as parquet\n",
    "location \"hdfs:///user/sergey/parquet/\";`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">`CREATE EXTERNAL TABLE IF NOT EXISTS bikes_parquet77 (\n",
    "station_id BIGINT,\n",
    "bikes_available BIGINT,\n",
    "docks_available BIGINT,\n",
    "time TIMESTAMP)\n",
    "row format delimited\n",
    "fields terminated by ','\n",
    "STORED AS PARQUET\n",
    "LOCATION '/user/sergey/parquet';`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pySpark (Spark 3.1.2)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
