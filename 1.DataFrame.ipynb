{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Create-Data-Frame\" data-toc-modified-id=\"Create-Data-Frame-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Create Data Frame</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1.-createDataFrame-from-Python-lists\" data-toc-modified-id=\"1.-createDataFrame-from-Python-lists-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>1. createDataFrame from Python lists</a></span></li><li><span><a href=\"#2.-createDataFrame-from-Python-DataFrame\" data-toc-modified-id=\"2.-createDataFrame-from-Python-DataFrame-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>2. createDataFrame from Python DataFrame</a></span></li><li><span><a href=\"#3.-Create-Data-Frame-by-read-method\" data-toc-modified-id=\"3.-Create-Data-Frame-by-read-method-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>3. Create Data Frame by <code>read</code> method</a></span></li><li><span><a href=\"#4.-Create-Data-Frame-by-rdd.toDF()-method\" data-toc-modified-id=\"4.-Create-Data-Frame-by-rdd.toDF()-method-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>4. Create Data Frame by <code>rdd.toDF()</code> method</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Type-inference-for-dummy-data\" data-toc-modified-id=\"4.1-Type-inference-for-dummy-data-1.0.4.1\"><span class=\"toc-item-num\">1.0.4.1&nbsp;&nbsp;</span>4.1 Type inference for dummy data</a></span></li><li><span><a href=\"#4.2-Type-inference-for-real-data\" data-toc-modified-id=\"4.2-Type-inference-for-real-data-1.0.4.2\"><span class=\"toc-item-num\">1.0.4.2&nbsp;&nbsp;</span>4.2 Type inference for real data</a></span></li></ul></li><li><span><a href=\"#5.-Generate-Data-Frame-with-sql-functions\" data-toc-modified-id=\"5.-Generate-Data-Frame-with-sql-functions-1.0.5\"><span class=\"toc-item-num\">1.0.5&nbsp;&nbsp;</span>5. Generate Data Frame with sql functions</a></span></li></ul></li></ul></li><li><span><a href=\"#Spark-Data-Frame-actions\" data-toc-modified-id=\"Spark-Data-Frame-actions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Spark Data Frame actions</a></span></li><li><span><a href=\"#Spark-Data-Frame-methods\" data-toc-modified-id=\"Spark-Data-Frame-methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Spark Data Frame methods</a></span></li><li><span><a href=\"#Spark-Data-Frame-transformations\" data-toc-modified-id=\"Spark-Data-Frame-transformations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Spark Data Frame transformations</a></span></li><li><span><a href=\"#Spark-df.groupBy(*cols)-transformation\" data-toc-modified-id=\"Spark-df.groupBy(*cols)-transformation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Spark df.groupBy(*cols) transformation</a></span></li><li><span><a href=\"#PySpark-SQL-Functions\" data-toc-modified-id=\"PySpark-SQL-Functions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>PySpark SQL Functions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T17:54:12.216410Z",
     "start_time": "2022-02-26T17:54:07.773164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.\n",
    "         builder.\n",
    "         appName(\"test\").\n",
    "         enableHiveSupport().\n",
    "         config('spark.sql.warehouse.dir','/user/hive/warehouse').\n",
    "#          config(\"hive.metastore.uris\",\"thrift://localhost:9083\").  # deprecated\n",
    "         getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T17:55:37.193604Z",
     "start_time": "2022-02-26T17:55:37.161553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '33273'),\n",
       " ('spark.app.name', 'test'),\n",
       " ('spark.app.startTime', '1645898049920'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.199.77'),\n",
       " ('spark.hadoop.fs.default.name', 'hdfs://localhost:9000'),\n",
       " ('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.hadoop.fs.defaultFS', 'hdfs://localhost:9000'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1645898050735'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T18:00:04.903593Z",
     "start_time": "2022-02-26T18:00:04.896197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.isModifiable('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T17:55:46.541977Z",
     "start_time": "2022-02-26T17:55:46.534911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T18:02:48.682212Z",
     "start_time": "2022-02-26T18:02:45.340607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://0.0.0.0:9000/user/hive/warehouse')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:46.463995Z",
     "start_time": "2021-12-06T14:08:45.994702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='cbr', description='', locationUri='hdfs://localhost:9000/user/hive/warehouse/cbr.db'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://localhost:9000/user/hive/warehouse'),\n",
       " Database(name='hive_essentials', description='', locationUri='hdfs://localhost:9000/user/hive/warehouse/hive_essentials.db'),\n",
       " Database(name='pluralsight', description='', locationUri='hdfs://localhost:9000/user/hive/warehouse/pluralsight.db'),\n",
       " Database(name='rostel', description='', locationUri='hdfs://localhost:9000/user/hive/warehouse/rostel.db'),\n",
       " Database(name='rostel_test', description='', locationUri='hdfs://localhost:9000/user/hive/warehouse/rostel_test.db')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists cbr;\")\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T18:06:40.790688Z",
     "start_time": "2022-02-26T18:06:40.783812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T18:11:21.089264Z",
     "start_time": "2022-02-26T18:11:20.751918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|     test|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Frame\n",
    "\n",
    "###  1. createDataFrame from Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T06:47:36.650295Z",
     "start_time": "2022-02-06T06:47:35.563554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 64|\n",
      "| Liz| 32|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Bob', 64), ('Liz', 32)]\n",
    "sdf = spark.createDataFrame(data=data, schema=\"name string, age int\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T06:49:26.084451Z",
     "start_time": "2022-02-06T06:49:25.972141Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"temp_view\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T06:51:51.625296Z",
     "start_time": "2022-02-06T06:51:51.547546Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.createOrReplaceGlobalTempView(\"global_temp_view\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T06:56:44.831561Z",
     "start_time": "2022-02-06T06:56:44.587549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 64|\n",
      "| Liz| 32|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a dedicated databases\n",
    "\n",
    "spark.sql(\"select * from global_temp.global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either list or tuple is accepted as a `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:47.867179Z",
     "start_time": "2021-12-06T14:08:47.564634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 64|\n",
      "| Liz| 32|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data=data,schema=('name','age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. createDataFrame from Python DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:48.150425Z",
     "start_time": "2021-12-06T14:08:47.871359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 70|\n",
      "| Liz| 35|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'name':['Bob','Liz'],'age':[70,35]}, columns=['name', 'age'])\n",
    "sdf = spark.createDataFrame(df)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Create Data Frame by `read` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:48.296545Z",
     "start_time": "2021-12-06T14:08:48.152026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:48.454307Z",
     "start_time": "2021-12-06T14:08:48.299001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 /home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:49.199625Z",
     "start_time": "2021-12-06T14:08:48.457519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"file:///home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv\"\n",
    "sdf = spark.read.csv(path, header=True, inferSchema = True) \n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:49.206893Z",
     "start_time": "2021-12-06T14:08:49.202332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:49.552012Z",
     "start_time": "2021-12-06T14:08:49.208701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"file:///home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv\"\n",
    "sdf = spark.read.format(\"csv\").load(path, header=True, inferSchema = True) \n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:49.557205Z",
     "start_time": "2021-12-06T14:08:49.553660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:50.035312Z",
     "start_time": "2021-12-06T14:08:49.558976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:50.168612Z",
     "start_time": "2021-12-06T14:08:50.037407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|        null|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|        null|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|        null|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|        null|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|        null|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"file:///home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/201408_station_data.csv\"\n",
    "\n",
    "customSchema = StructType([\n",
    "        StructField(\"station_id\", IntegerType(),True),\n",
    "        StructField(\"name\", StringType(),True),\n",
    "        StructField(\"lat\", DoubleType(),True),\n",
    "        StructField(\"long\", DoubleType(),True),\n",
    "        StructField(\"dockcount\", IntegerType(),True),\n",
    "        StructField(\"landmark\", StringType(),True),\n",
    "        StructField(\"installation\", DateType(),True) # fails here\n",
    "    ])\n",
    "\n",
    "sdf = spark.read.option('samplingRatio', .5).csv(path, header=True, schema=customSchema)\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:50.173302Z",
     "start_time": "2021-12-06T14:08:50.170167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:50.917216Z",
     "start_time": "2021-12-06T14:08:50.174756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     landmark|count|\n",
      "+-------------+-----+\n",
      "|    Palo Alto|    5|\n",
      "|San Francisco|   35|\n",
      "|     San Jose|   16|\n",
      "| Redwood City|    7|\n",
      "|Mountain View|    7|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy(\"landmark\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:51.393608Z",
     "start_time": "2021-12-06T14:08:50.918989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     landmark|count|\n",
      "+-------------+-----+\n",
      "|    Palo Alto|    2|\n",
      "|San Francisco|   14|\n",
      "|     San Jose|    5|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2 = sdf.sampleBy('landmark', fractions={'Palo Alto': .5, 'San Francisco': .5, 'San Jose': .5}, seed=42)\n",
    "sdf2.groupBy('landmark').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:52.257181Z",
     "start_time": "2021-12-06T14:08:51.395330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_id,name,lat,long,dockcount,landmark,installation\r",
      "\r\n",
      "2,San Jose Diridon Caltrain Station,37.329732,-121.901782,27,San Jose,8/6/2013\r",
      "\r\n",
      "3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013\r",
      "\r\n",
      "4,Santa Clara at Almaden,37.333988,-121.894902,11,San Jose,8/6/2013\r",
      "\r\n",
      "5,Adobe on Almaden,37.331415,-121.8932,19,San Jose,8/5/2013\r",
      "\r\n",
      "6,San Pedro Square,37.336721,-121.894074,15,San Jose,8/7/2013\r",
      "\r\n",
      "7,Paseo de San Antonio,37.333798,-121.886943,15,San Jose,8/7/2013\r",
      "\r\n",
      "8,San Salvador at 1st,37.330165,-121.885831,15,San Jose,8/5/2013\r",
      "\r\n",
      "9,Japantown,37.348742,-121.894715,15,San Jose,8/5/2013\r",
      "\r\n",
      "10,San Jose City Hall,37.337391,-121.886995,15,San Jose,8/6/2013\r",
      "\r\n",
      "11,MLK Library,37.335885,-121.88566,19,San Jose,8/6/2013\r",
      "\r\n",
      "12,SJSU 4th at San Carlos,37.332808,-121.883891,19,San Jose,8/7/2013\r",
      "\r\n",
      "13,St James Park,37.339301,-121.889937,15,San Jose,8/6/2013\r",
      "\r\n",
      "14,Arena Green / SAP Center,37.332692,-121.900084,19,San Jose,8/5/2013\r",
      "\r\n",
      "16,SJSU - San Salvador at 9th,37.333955,-121.877349,15,San Jose,8/7/2013\r",
      "\r\n",
      "21,Franklin at Maple,37.481758,-122"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:52.352944Z",
     "start_time": "2021-12-06T14:08:52.260624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|  2013-08-06|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|  2013-08-05|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|  2013-08-06|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customSchema2 = \"station_id int, name string, lat double, long double, dockcount int, landmark string, installation date\"\n",
    "sdf = (\n",
    "    spark.\n",
    "    read.\n",
    "    option(\"dateFormat\", \"M/d/yyyy\").\n",
    "    csv(path=path, header=True, schema=customSchema2)\n",
    "       )\n",
    "sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:52.358237Z",
     "start_time": "2021-12-06T14:08:52.354629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:53.892011Z",
     "start_time": "2021-12-06T14:08:52.359868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|  2013-08-06|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|  2013-08-05|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|  2013-08-06|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.write.save(path=\"sdf.parquet\", format=\"parquet\", mode=\"overwrite\")\n",
    "sdf2 = spark.read.load(\"sdf.parquet\", format=\"parquet\", header=True, inferSchema=True)\n",
    "sdf2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:53.900387Z",
     "start_time": "2021-12-06T14:08:53.895335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sql.read.text` always returns 1 column named \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:54.124143Z",
     "start_time": "2021-12-06T14:08:53.902789Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|           FILE LIST|\n",
      "|1) 201408_status_...|\n",
      "|2) 201408_station...|\n",
      "|3) 201408_trip_da...|\n",
      "|4) 201408_weather...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path2 = \"file:///home/sergey/Py_SparkDataFrame_edx_CS105_CS110_CS120/data/README.txt\"\n",
    "df3_2 = spark.read.text(path2)\n",
    "df3_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:54.230543Z",
     "start_time": "2021-12-06T14:08:54.127716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|           FILE LIST|\n",
      "|1) 201408_status_...|\n",
      "|2) 201408_station...|\n",
      "|3) 201408_trip_da...|\n",
      "|4) 201408_weather...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_2 = (spark.\n",
    "         read.\n",
    "         option(\"header\", \"true\"). # doesn't work\n",
    "         text(path2))\n",
    "df3_2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Create Data Frame by `rdd.toDF()` method\n",
    "\n",
    "**Another name for `rdd.toDF()` is type inference by Reflection**\n",
    "\n",
    "#### 4.1 Type inference for dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:08:56.610916Z",
     "start_time": "2021-12-06T14:08:56.604122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', 64), ('Liz', 32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:10:33.162627Z",
     "start_time": "2021-12-06T14:10:33.152306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.199.77:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=test>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:10:49.707754Z",
     "start_time": "2021-12-06T14:10:49.195098Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 64|\n",
      "| Liz| 32|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "df4 = rdd.toDF(schema=['name','age'])\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T12:25:18.936Z"
    }
   },
   "outputs": [],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Type inference for real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Strip header**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:15:30.438926Z",
     "start_time": "2021-12-06T14:15:30.265800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,San Jose Diridon Caltrain Station,37.329732,-121.901782,27,San Jose,8/6/2013',\n",
       " '3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013',\n",
       " '4,Santa Clara at Almaden,37.333988,-121.894902,11,San Jose,8/6/2013']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(path)\n",
    "rddWoHeader = rdd.zipWithIndex().filter(lambda x: x[1] > 0 ).map(lambda x: x[0])\n",
    "rddWoHeader.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:14:03.353547Z",
     "start_time": "2021-12-06T14:14:03.191458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,San Jose Diridon Caltrain Station,37.329732,-121.901782,27,San Jose,8/6/2013',\n",
       " '3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013',\n",
       " '4,Santa Clara at Almaden,37.333988,-121.894902,11,San Jose,8/6/2013']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = sc.textFile(path)\n",
    "_.zipWithIndex().filter(lambda x: x[1]>0).map(lambda x: x[0]).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Create `rowRDD`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:15:33.895029Z",
     "start_time": "2021-12-06T14:15:33.887320Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime as dt\n",
    "rowRDD = (rddWoHeader\n",
    "          .map(lambda x: x.split(\",\"))\n",
    "          .map(lambda x: Row(id_=int(x[0])\n",
    "                             ,name = x[1]\n",
    "                             ,lat = float(x[2])\n",
    "                             ,long = float(x[3])\n",
    "                             ,dockcount = int(x[4])\n",
    "                             ,landmark = x[5]\n",
    "                             ,installation = dt.strptime(x[6], '%m/%d/%Y')\n",
    "                            )\n",
    "              )\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Cast `rowRDD` to DataFrame with `rdd.toDF()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:40:02.565593Z",
     "start_time": "2021-12-06T14:40:02.333842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+-----------+---------+--------+-------------------+\n",
      "|id_|                name|      lat|       long|dockcount|landmark|       installation|\n",
      "+---+--------------------+---------+-----------+---------+--------+-------------------+\n",
      "|  2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|2013-08-06 00:00:00|\n",
      "|  3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|2013-08-05 00:00:00|\n",
      "|  4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|2013-08-06 00:00:00|\n",
      "|  5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|2013-08-05 00:00:00|\n",
      "|  6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|2013-08-07 00:00:00|\n",
      "+---+--------------------+---------+-----------+---------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = rowRDD.toDF()\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:40:07.851899Z",
     "start_time": "2021-12-06T14:40:07.845793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: long (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:16:48.599841Z",
     "start_time": "2021-12-06T14:16:48.499559Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:40:22.762819Z",
     "start_time": "2021-12-06T14:40:22.628811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+-----------+---------+--------+-------------------+\n",
      "|id_|                name|      lat|       long|dockcount|landmark|       installation|\n",
      "+---+--------------------+---------+-----------+---------+--------+-------------------+\n",
      "|  2|San Jose Diridon ...|37.329732|-121.901782|      3.3|San Jose|2013-08-06 00:00:00|\n",
      "|  3|San Jose Civic Ce...|37.330698|-121.888979|     2.71|San Jose|2013-08-05 00:00:00|\n",
      "|  4|Santa Clara at Al...|37.333988|-121.894902|      2.4|San Jose|2013-08-06 00:00:00|\n",
      "|  5|    Adobe on Almaden|37.331415|  -121.8932|     2.94|San Jose|2013-08-05 00:00:00|\n",
      "|  6|    San Pedro Square|37.336721|-121.894074|     2.71|San Jose|2013-08-07 00:00:00|\n",
      "+---+--------------------+---------+-----------+---------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark only understands core Python\n",
    "import math\n",
    "func = F.udf(lambda x: round(math.log(x),2))\n",
    "df3.withColumn(\"dockcount\",func(df3[\"dockcount\"])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate Data Frame with sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen df with id col\n",
    "df = spark.range(1,5)\n",
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:23:16.667521Z",
     "start_time": "2021-12-06T14:23:16.490282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------------------+\n",
      "| id|             rand1|              rand2|\n",
      "+---+------------------+-------------------+\n",
      "|  0|0.1709497137955568| 0.8894415403143504|\n",
      "|  1|0.8051143958005459| 0.9658033539477212|\n",
      "|  2|0.5775925576589018|0.18783354174799438|\n",
      "|  3|0.9476047869880925|0.28489885393300474|\n",
      "|  4|   0.2093704977577| 0.5207165345237337|\n",
      "+---+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "df5 = (spark.range(0, 1000).\n",
    "       withColumn('rand1', rand(seed=10)).\n",
    "       withColumn('rand2', rand(seed=27)))\n",
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:23:43.883726Z",
     "start_time": "2021-12-06T14:23:43.804972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Frame actions\n",
    "\n",
    "- df.toPandas()\n",
    "- df.show()\n",
    "- df.take(n)\n",
    "- df.collect()\n",
    "- df.count()\n",
    "- df.describe()\n",
    "\n",
    "# Spark Data Frame methods\n",
    "- df.cache()  # `.cache()` is lazy, i.e. an action must occur before actual caching happens\n",
    "- df.unpersists() # `uncache`\n",
    "- df.printSchema()\n",
    "- `df.select('name1', function('name2'), df.name3, ...., 'nameN')`\n",
    "- df.drop('nameN')\n",
    "- df.select(function('nameN').alias('anotherName')) # udf or imported func\n",
    "- `df.sample(withReplacement=False, fraction=0.10)`\n",
    "\n",
    "# Spark Data Frame transformations\n",
    "\n",
    "- df.filter(func) - returns rows where func evaluates to \"True\"\n",
    "- df.where(func) - \"where\" is alias for \"filter\"\n",
    "- df.distinct() - distinct rows\n",
    "- df.orderBy(\\*colS)\n",
    "- df.sort(\\*colS)\n",
    "- df.explode(coL) - returns new row for each element in a specified column\n",
    "\n",
    "\n",
    "# Spark df.groupBy(*cols) transformation\n",
    "\n",
    "- df.groupBy(\\*cols).agg(avg, min, max, count, sum)\n",
    "- df.groupBy(\\*cols).avg()\n",
    "- df.groupBy(\\*cols).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:36:55.616077Z",
     "start_time": "2021-12-06T14:36:55.560481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('Bob', 64), ('Liz', 32)]\n",
    "sdf = spark.createDataFrame(data=data, schema=\"name string, age int\")\n",
    "ageCol = sdf.age\n",
    "ageCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:40:33.613382Z",
     "start_time": "2021-12-06T14:40:33.607781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: long (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:44:52.321526Z",
     "start_time": "2021-12-06T14:44:52.312287Z"
    }
   },
   "outputs": [],
   "source": [
    "df33 = df3.withColumnRenamed(\"id_\", \"station_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:45:09.816814Z",
     "start_time": "2021-12-06T14:45:09.706314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|station_id|dockcount|\n",
      "+----------+---------+\n",
      "|         2|       27|\n",
      "|         3|       15|\n",
      "|         4|       11|\n",
      "+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df33.select('station_id','dockcount')\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark SQL Functions  \n",
    "\n",
    "- df.select(concat(\\*colNames))\n",
    "- df.select(concat(\\*colNames, lit('string')) # `lit` Creates a Column of literal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:45:25.867846Z",
     "start_time": "2021-12-06T14:45:25.748511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|(dockcount + 10)|\n",
      "+----------------+\n",
      "|              37|\n",
      "|              25|\n",
      "|              21|\n",
      "|              29|\n",
      "|              25|\n",
      "|              25|\n",
      "|              25|\n",
      "|              25|\n",
      "|              25|\n",
      "|              29|\n",
      "|              29|\n",
      "|              25|\n",
      "|              29|\n",
      "|              25|\n",
      "|              25|\n",
      "|              35|\n",
      "|              25|\n",
      "|              25|\n",
      "|              25|\n",
      "|              25|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df33.select(df33.dockcount+10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:45:30.100175Z",
     "start_time": "2021-12-06T14:45:29.957976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   avg(dockcount)|\n",
      "+-----------------+\n",
      "|17.65714285714286|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df33.select(avg(\"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:46:27.789743Z",
     "start_time": "2021-12-06T14:46:27.782148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArrayType',\n",
       " 'AtomicType',\n",
       " 'BinaryType',\n",
       " 'BooleanType',\n",
       " 'ByteType',\n",
       " 'CloudPickleSerializer',\n",
       " 'DataType',\n",
       " 'DataTypeSingleton',\n",
       " 'DateConverter',\n",
       " 'DateType',\n",
       " 'DatetimeConverter',\n",
       " 'DecimalType',\n",
       " 'DoubleType',\n",
       " 'FloatType',\n",
       " 'FractionalType',\n",
       " 'IntegerType',\n",
       " 'IntegralType',\n",
       " 'JavaClass',\n",
       " 'LongType',\n",
       " 'MapType',\n",
       " 'NullType',\n",
       " 'NumericType',\n",
       " 'Row',\n",
       " 'ShortType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'StructField',\n",
       " 'StructType',\n",
       " 'TimestampType',\n",
       " 'UserDefinedType',\n",
       " '_FIXED_DECIMAL',\n",
       " '_acceptable_types',\n",
       " '_all_atomic_types',\n",
       " '_all_complex_types',\n",
       " '_array_signed_int_typecode_ctype_mappings',\n",
       " '_array_type_mappings',\n",
       " '_array_unsigned_int_typecode_ctype_mappings',\n",
       " '_atomic_types',\n",
       " '_create_converter',\n",
       " '_create_row',\n",
       " '_create_row_inbound_converter',\n",
       " '_has_nulltype',\n",
       " '_infer_schema',\n",
       " '_infer_type',\n",
       " '_int_size_to_type',\n",
       " '_make_type_verifier',\n",
       " '_merge_type',\n",
       " '_need_converter',\n",
       " '_parse_datatype_json_string',\n",
       " '_parse_datatype_json_value',\n",
       " '_parse_datatype_string',\n",
       " '_test',\n",
       " '_type_mappings',\n",
       " '_typecode',\n",
       " 'array',\n",
       " 'base64',\n",
       " 'calendar',\n",
       " 'ctypes',\n",
       " 'datetime',\n",
       " 'decimal',\n",
       " 'dt',\n",
       " 'json',\n",
       " 're',\n",
       " 'register_input_converter',\n",
       " 'size',\n",
       " 'sys',\n",
       " 'time']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "dir_(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:47:33.914069Z",
     "start_time": "2021-12-06T14:47:33.594623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|length|\n",
      "+------+\n",
      "|     3|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "slen = udf(lambda s: len(s), IntegerType()) # Specify Return Type\n",
    "sdf.select(slen('name').alias('length')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:47:45.646734Z",
     "start_time": "2021-12-06T14:47:45.517666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|double_dockcount|\n",
      "+----------------+\n",
      "|              54|\n",
      "|              30|\n",
      "|              22|\n",
      "|              38|\n",
      "|              30|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "double_dock = udf(lambda x: 2*int(x), IntegerType())\n",
    "df3.select(double_dock('dockcount').alias('double_dockcount')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:47:48.824889Z",
     "start_time": "2021-12-06T14:47:48.715463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|double_dockcount|\n",
      "+----------------+\n",
      "|              54|\n",
      "|              30|\n",
      "|              22|\n",
      "|              38|\n",
      "|              30|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select((2*df3.dockcount).alias('double_dockcount')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:47:55.368641Z",
     "start_time": "2021-12-06T14:47:55.194557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|        dockcount|\n",
      "+-------+-----------------+\n",
      "|  count|               70|\n",
      "|   mean|17.65714285714286|\n",
      "| stddev|4.010441857493953|\n",
      "|    min|               11|\n",
      "|    max|               27|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.describe('dockcount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:49:43.750828Z",
     "start_time": "2021-12-06T14:49:43.734701Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21860/3825025027.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dockcount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df3['dockcount'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:50:15.564032Z",
     "start_time": "2021-12-06T14:50:15.435977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|max(dockcount)|\n",
      "+--------------+\n",
      "|            27|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy().max('dockcount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:50:43.766925Z",
     "start_time": "2021-12-06T14:50:43.628619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| MX|\n",
      "+---+\n",
      "| 27|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "to_int = udf(lambda x: int(x), IntegerType())\n",
    "df3.select(max(to_int('dockcount')).alias(\"MX\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T12:25:19.062Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "to_int = udf(lambda x: int(x), IntegerType())\n",
    "df3.select(to_int('dockcount')).groupby().max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just enough !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:52:37.061331Z",
     "start_time": "2021-12-06T14:52:36.938617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|max(dockcount)|\n",
      "+--------------+\n",
      "|            27|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df3.select(max('dockcount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T14:52:46.696288Z",
     "start_time": "2021-12-06T14:52:46.682769Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21860/974238772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf22\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf22\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf22\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Single int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df22' is not defined"
     ]
    }
   ],
   "source": [
    "df22.select(df22.a, explode(df22.intlist).alias('Single int')).show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "300px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
